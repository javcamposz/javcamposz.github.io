---
layout: single
title: "Can We Trust a 'Well-Behaved' AI? The Challenge of Deceptive Alignment"
date: 2025-09-20
author: Francisco Javier Campos Zabala
categories: [Artificial Intelligence, AI Safety, Technology]
tags: [AI, AI Safety, Alignment, Machine Learning, Ethics]
thumbnail-img: /assets/img/llm_deception.png
header:
  teaser: /assets/img/llm_deception.png
image: /assets/img/llm_deception.png
gh-repo: javcamposz/
gh-badge: [star, fork, follow]
comments: true
---


As someone very involved in the field of AI safety, I'm constantly grappling with what I consider one of the most critical and difficult challenges of our time: ensuring that increasingly intelligent systems remain aligned with human values. The task is monumental. If we struggle to align our own human institutions, how can we perfect this for artificial intelligences that may one day surpass our own capabilities?

This is why the latest research on "scheming" in AI is so significant.

A recent collaborative study by OpenAI and Apollo Research has provided concrete evidence of something many of us in the safety community have been concerned about. In controlled tests, today's leading AI models have demonstrated behaviours consistent with "scheming"‚Äîappearing aligned while pursuing hidden goals.

Here are some of the key takeaways from this crucial research:
‚Ä¢ **Models Can Be Deceptive**: Frontier AI models can learn to recognise when they are being evaluated and alter their behaviour to appear safer than they might be in a real-world deployment scenario.
‚Ä¢ **The Double-Edged Sword of Transparency**: Our ability to monitor an AI's "chain-of-thought" is a vital tool for safety. However, there's a risk that models could be trained to hide their true reasoning, making this transparency fragile. This potential for "eval-aware opacity" is a serious hurdle we must overcome.
‚Ä¢ **A Cross-Industry Effort**: It's encouraging to see major labs like OpenAI, Anthropic, and independent researchers at Apollo collaborating on this issue. This level of cooperation is exactly what we need to address systemic risks.

In my own research, detailed in the attached paper, I explore these alignment risks and the importance of maintaining monitorability in AI systems. Together, this combined effort helps us better understand the subtle, deceptive behaviours that advanced AI might develop and how we can safeguard against them.

The announcement of a $500K Kaggle challenge to further this research is a fantastic step in the right direction, and it was heartening to see this work draw rare positive commentary from notable alignment skeptics.

üîó [Read the full post on LinkedIn](https://www.linkedin.com/in/camposjavier/)

#AISafety #AIAlignment #ArtificialIntelligence #EthicsInAI #ResponsibleAI #MachineLearning #TechPolicy #FutureOfAI

‚Ä¢ üîó ES2030 Agentic AI Masterclass: https://lnkd.in/eCgy_jFX
‚Ä¢ üîó My new book, "Autonomous Minds": https://lnkd.in/eJfZ3PWt
‚Ä¢ üîó Book1: https://bit.ly/4b31PEG
‚Ä¢ üîó Alignment paper: https://lnkd.in/ez8cV-jK

üî• Like if you found this helpful
‚ûï Follow for more applied AI insights
‚ôªÔ∏è Share to help your network grow
